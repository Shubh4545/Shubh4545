{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQDgoCHTFFtOfccaFUqQmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubh4545/Shubh4545/blob/main/nlp%20MCQ%20Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdyT5KiVl9ua",
        "outputId": "1f502ac3-efd7-4d4d-b472-4d45e92822e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mca_questions(context):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(context)\n",
        "    \n",
        "    # Preprocess the sentences\n",
        "    processed_sentences = preprocess_sentences(sentences)\n",
        "    \n",
        "    # Use TF-IDF to find the most important sentence\n",
        "    important_sentence = get_most_important_sentence(processed_sentences)\n",
        "    \n",
        "    # Generate multiple-choice questions based on the important sentence\n",
        "    mca_questions = generate_mca_questions(important_sentence)\n",
        "    \n",
        "    return mca_questions\n",
        "\n",
        "def preprocess_sentences(sentences):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_sentences = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence into words\n",
        "        words = nltk.word_tokenize(sentence.lower())\n",
        "        \n",
        "        # Remove stop words and lemmatize the words\n",
        "        words = [lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words]\n",
        "        \n",
        "        # Convert the words back to sentence\n",
        "        processed_sentence = ' '.join(words)\n",
        "        \n",
        "        processed_sentences.append(processed_sentence)\n",
        "    \n",
        "    return processed_sentences\n",
        "\n",
        "def get_most_important_sentence(processed_sentences):\n",
        "    # Initialize TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    \n",
        "    # Compute TF-IDF matrix\n",
        "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
        "    \n",
        "    # Compute cosine similarity between sentences\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "    \n",
        "    # Find the most important sentence based on cosine similarity\n",
        "    important_sentence_index = similarity_matrix.sum(axis=1).argmax()\n",
        "    \n",
        "    return processed_sentences[important_sentence_index]\n",
        "\n",
        "\n",
        "\n",
        "def generate_mca_questions(important_sentence):\n",
        "    # Initialize OpenAI GPT-3.5 pipeline for question generation\n",
        "    generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')\n",
        "\n",
        "    # Generate multiple-choice questions based on the important sentence\n",
        "    num_questions = 3\n",
        "    questions = []\n",
        "    for _ in range(num_questions):\n",
        "        # Generate the question\n",
        "        question = generator(important_sentence, max_length=30, num_return_sequences=1)[0]['generated_text']\n",
        "        question = question.split('?')[0] + '?'\n",
        "\n",
        "        # Generate the options\n",
        "        options = generate_options(important_sentence)\n",
        "        \n",
        "        # Randomly select the correct answers\n",
        "        correct_answers = random.sample(options, 2)\n",
        "        \n",
        "        # Create the correct options string\n",
        "        correct_options = \"Correct Options: \"\n",
        "        for i, answer in enumerate(correct_answers, 1):\n",
        "            correct_options += f\"({chr(97 + i)}) {answer} \"\n",
        "        \n",
        "        # Shuffle the options\n",
        "        random.shuffle(options)\n",
        "        \n",
        "        # Create the question string with shuffled options\n",
        "        question_with_options = question + '\\n'\n",
        "        for i, option in enumerate(options, 1):\n",
        "            question_with_options += f\"{chr(97 + i)}. {option}\\n\"\n",
        "        \n",
        "        # Append the question to the list\n",
        "        questions.append((question_with_options, correct_options))\n",
        "    \n",
        "    return questions\n",
        "\n",
        "\n",
        "def generate_options(important_sentence):\n",
        "    # Split the important sentence into words\n",
        "    words = important_sentence.split()\n",
        "    \n",
        "    # Shuffle the words\n",
        "    random.shuffle(words)\n",
        "    \n",
        "    # Select two words as correct answers\n",
        "    correct_answers = random.sample(words, 2)\n",
        "    \n",
        "    # Create options with two correct answers and two incorrect answers\n",
        "    options = correct_answers + random.sample([word for word in words if word not in correct_answers], 2)\n",
        "    \n",
        "    # Shuffle the options\n",
        "    random.shuffle(options)\n",
        "    \n",
        "    return options\n",
        "\n",
        "# Example usage\n",
        "context = \"Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that, through cellular respiration, can later be released to fuel the organism's activities. Some of this chemical energy is stored in carbohydrate molecules, such as sugars and starches, which are synthesized from carbon dioxide and water. Most plants, algae, and cyanobacteria perform photosynthesis.\"\n",
        "\n",
        "mca_questions = get_mca_questions(context)\n",
        "for i, (question, correct_options) in enumerate(mca_questions, 1):\n",
        "    print(f\"Q{i}: {question}\")\n",
        "    print(f\"{correct_options}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct_f3lSJepAK",
        "outputId": "0ea2ceae-6911-4503-a55b-2cf02ed8e10f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: photosynthesis process used plant organism convert light energy chemical energy cellular respiration later released fuel organism activity and maintenance. These processes are important and must be maintained?\n",
            "b. cellular\n",
            "c. respiration\n",
            "d. organism\n",
            "e. chemical\n",
            "\n",
            "Correct Options: (b) respiration (c) chemical \n",
            "\n",
            "Q2: photosynthesis process used plant organism convert light energy chemical energy cellular respiration later released fuel organism activity. This phenomenon is an important process to plant life process?\n",
            "b. convert\n",
            "c. organism\n",
            "d. activity\n",
            "e. chemical\n",
            "\n",
            "Correct Options: (b) activity (c) organism \n",
            "\n",
            "Q3: photosynthesis process used plant organism convert light energy chemical energy cellular respiration later released fuel organism activity carbon dioxide CO2\n",
            "\n",
            "This will cause to make?\n",
            "b. process\n",
            "c. activity\n",
            "d. cellular\n",
            "e. organism\n",
            "\n",
            "Correct Options: (b) cellular (c) activity \n",
            "\n"
          ]
        }
      ]
    }
  ]
}